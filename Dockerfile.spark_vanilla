# Dockerfile.spark

FROM openjdk:11-jre-slim-bullseye 
ARG SPARK_VERSION=3.5.6
ARG HADOOP_VERSION=3 

ENV SPARK_HOME=/opt/spark
ENV PATH="${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"

USER root
RUN mkdir -p /opt/spark/apps
COPY airflow_home/scripts/spark_consumer_kafka.py /opt/spark/apps/spark_consumer_kafka.py

# Install necessary tools and upgrade pip/setuptools
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    bash \
    python3 \
    python3-pip \
    netcat-openbsd \
    build-essential libffi-dev \
    wget \
    procps \
    && rm -rf /var/lib/apt/lists/* \
    && pip3 install --no-cache-dir --upgrade pip setuptools

# Download and extract Apache Spark
RUN curl -LO "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /tmp/ \
    && mv /tmp/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}/* ${SPARK_HOME}/ \     
    && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && rmdir /tmp/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} 


# Download JDBC and Kafka JARs directly into Spark's jars directory
RUN mkdir -p ${SPARK_HOME}/jars/
RUN wget https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.0.33/mysql-connector-j-8.0.33.jar -P ${SPARK_HOME}/jars/
RUN wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.5.0/spark-streaming-kafka-0-10-assembly_2.12-3.5.0.jar -P ${SPARK_HOME}/jars/
RUN wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.0/kafka-clients-3.5.0.jar -P ${SPARK_HOME}/jars/

# Install PySpark
RUN pip3 install --no-cache-dir pyspark==${SPARK_VERSION}

# Set working directory
WORKDIR ${SPARK_HOME}